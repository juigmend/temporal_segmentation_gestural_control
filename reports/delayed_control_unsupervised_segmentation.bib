%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Juan Mendoza at 2023-01-23 13:21:58 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{McPherson_etal_2016,
	address = {Brisbane, Australia},
	author = {McPherson, Andrew and Jack, Robert and Moro, Giulio},
	booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	date-added = {2023-01-23 13:06:22 +0200},
	date-modified = {2023-01-23 13:06:48 +0200},
	doi = {10.5281/zenodo.3964611},
	isbn = {978-1-925455-13-7},
	issn = {2220-4806},
	pages = {20--25},
	publisher = {Queensland Conservatorium Griffith University},
	title = {Action-Sound Latency: Are Our Tools Fast Enough?},
	track = {Papers},
	url = {http://www.nime.org/proceedings/2016/nime2016_paper0005.pdf},
	year = {2016},
	bdsk-url-1 = {http://www.nime.org/proceedings/2016/nime2016_paper0005.pdf},
	bdsk-url-2 = {https://doi.org/10.5281/zenodo.3964611}}

@phdthesis{Visi_2017,
	author = {Visi, Federico},
	date-added = {2023-01-23 12:56:14 +0200},
	date-modified = {2023-01-23 12:57:17 +0200},
	school = {Interdisciplinary Centre for Computer Music Research, Plymouth University},
	title = {Methods and Technologies for the Analysis and Interactive Use of Body Movements in Instrumental Music Performance},
	year = {2017}}

@inproceedings{Tahiroglu_etal_2013,
	address = {Daejeon, Republic of Korea},
	author = {Tahiro{\u g}lu, Koray and Correia, Nuno N. and Espada, Miguel},
	booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	date-added = {2023-01-23 12:44:55 +0200},
	date-modified = {2023-01-23 12:45:06 +0200},
	doi = {10.5281/zenodo.1178666},
	issn = {2220-4806},
	keywords = {Affordances, collaboration, social interaction, mobile music, extended system, NIME},
	month = may,
	pages = {35--40},
	publisher = {Graduate School of Culture Technology, KAIST},
	title = {PESI Extended System: In Space, On Body, with 3 Musicians},
	url = {http://www.nime.org/proceedings/2013/nime2013_97.pdf},
	year = {2013},
	bdsk-url-1 = {http://www.nime.org/proceedings/2013/nime2013_97.pdf},
	bdsk-url-2 = {https://doi.org/10.5281/zenodo.1178666}}

@inproceedings{Staudt_etal_2022,
	author = {Staudt, Pascal; Sarig{\"o}l, Emre and Lussana, Marcello and Rizzonelli, Marta and Hyun Kim, Jin},
	booktitle = {Proceedings of the Conference on Sonification of Health and Environmental Data (SoniHED 2022).},
	date-added = {2023-01-23 12:42:41 +0200},
	date-modified = {2023-01-23 13:20:49 +0200},
	doi = {10.5281/zenodo.7243901},
	editor = {Pauletto, S. and Delle Monache, S. and Selfridge, R.},
	month = oct,
	note = {ISBN: 978-91-8040-358-0},
	publisher = {Zenodo},
	title = {Automatic Classification of Interactive Gestures for Inter-Body Proximity Sonification},
	url = {https://doi.org/10.5281/zenodo.7243901},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.7243901}}

@article{Burger_Toiviainen_2020,
	author = {Burger, Birgitta and Toiviainen, Petri},
	date-added = {2023-01-23 12:40:01 +0200},
	date-modified = {2023-01-23 12:40:11 +0200},
	journal = {Musicae Scientiae},
	number = {2},
	pages = {186--205},
	publisher = {SAGE Publications Sage UK: London, England},
	title = {Embodiment in electronic dance music: Effects of musical content and structure on body movement},
	volume = {24},
	year = {2020}}

@webpage{Wekinator_website,
	author = {Fiebrink, Rebecca},
	date-added = {2023-01-23 12:37:57 +0200},
	date-modified = {2023-01-23 13:18:34 +0200},
	note = {http://www.wekinator.org/},
	url = {http://www.wekinator.org/}}

@inproceedings{Fiebrink_etal_2009,
	address = {Pittsburgh, PA, United States},
	author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R.},
	booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	date-added = {2023-01-23 12:37:10 +0200},
	date-modified = {2023-01-23 12:37:22 +0200},
	doi = {10.5281/zenodo.1177513},
	issn = {2220-4806},
	keywords = {Machine learning, mapping, tools.},
	pages = {280--285},
	title = {A Meta-Instrument for Interactive, On-the-Fly Machine Learning},
	url = {http://www.nime.org/proceedings/2009/nime2009_280.pdf},
	year = {2009},
	bdsk-url-1 = {http://www.nime.org/proceedings/2009/nime2009_280.pdf},
	bdsk-url-2 = {https://doi.org/10.5281/zenodo.1177513}}

@article{Mendoza_2022,
	author = {Mendoza, Juan Ignacio},
	date-added = {2023-01-23 12:33:34 +0200},
	date-modified = {2023-01-23 13:14:20 +0200},
	journal = {Human Technology},
	number = {3},
	pages = {250--266},
	title = {Segmentation boundaries in accelerometer data of arm motion induced by music: Online computation and perceptual assessment},
	volume = {18},
	year = {2022}}

@inproceedings{Mendoza_etal_2022,
	author = {Mendoza, Juan Ignacio and Danso, Andrew and Luck, Geoff and Rantalainen, Timo and Palmberg, Lotta and Chastin, Sebastien},
	booktitle = {Conference on the Sonification of Health and Environmental Data. ISBN: 978-91-8040-358-0},
	date-added = {2023-01-23 12:32:14 +0200},
	date-modified = {2023-01-23 13:21:56 +0200},
	doi = {10.5281/zenodo.7243875},
	editor = {Pauletto, S. and Delle Monache, S. and Selfridge, R.},
	month = {oct},
	note = {ISBN: 978-91-8040-358-0},
	organization = {KTH Royal Institute of Technology},
	title = {Musification of Accelerometry Data Towards Raising Awareness of Physical Activity},
	url = {https://doi.org/10.5281/zenodo.7243875},
	year = {2022}}

@inproceedings{Murad_etal_2017,
	author = {Murad, Dania and Ye, Fenyi and Barone, Michael and Wang, Ye},
	booktitle = {2017 international conference on orange technologies (icot)},
	date-added = {2023-01-23 12:30:29 +0200},
	date-modified = {2023-01-23 12:30:39 +0200},
	organization = {IEEE},
	pages = {87--90},
	title = {Motion initiated music ensemble with sensors for motor rehabilitation},
	year = {2017}}

@inproceedings{Merril_Paradiso_2005,
	author = {Merrill, David J and Paradiso, Joseph A},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems, Extended Abstracts},
	date-added = {2023-01-23 12:29:49 +0200},
	date-modified = {2023-01-23 12:30:00 +0200},
	pages = {2152--2161},
	title = {Personalization, expressivity, and learnability of an implicit mapping strategy for physical interfaces},
	year = {2005}}

@inproceedings{Bevilacqua_etal_2010,
	author = {Bevilacqua, Fr{\'e}d{\'e}ric and Zamborlin, Bruno and Sypniewski, Anthony and Schnell, Norbert and Gu{\'e}dy, Fabrice and Rasamimanana, Nicolas},
	booktitle = {Gesture in Embodied Communication and Human-Computer Interaction: 8th International Gesture Workshop, GW 2009, Bielefeld, Germany, February 25-27, 2009, Revised Selected Papers 8},
	date-added = {2023-01-23 12:29:08 +0200},
	date-modified = {2023-01-23 12:29:16 +0200},
	organization = {Springer},
	pages = {73--84},
	title = {Continuous realtime gesture following and recognition},
	year = {2010}}

@inproceedings{Gillian_etal_2011,
	author = {Gillian, Nicholas and Knapp, Benjamin and O'modhrain, Sile},
	booktitle = {Nime},
	date-added = {2023-01-23 12:28:14 +0200},
	date-modified = {2023-01-23 12:28:26 +0200},
	pages = {337--342},
	title = {Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping.},
	year = {2011}}

@phdthesis{Gillian_2011,
	author = {Gillian, Nicholas Edward},
	date-added = {2023-01-23 12:26:52 +0200},
	date-modified = {2023-01-23 12:27:43 +0200},
	school = {Citeseer},
	title = {Gesture recognition for musician computer interaction},
	year = {2011}}

@article{Jack_etal_2018,
	author = {Jack, Robert H and Mehrabi, Adib and Stockman, Tony and McPherson, Andrew},
	date-added = {2023-01-23 12:24:03 +0200},
	date-modified = {2023-01-23 12:24:17 +0200},
	journal = {Music Perception: An Interdisciplinary Journal},
	number = {1},
	pages = {109--128},
	publisher = {University of California Press USA},
	title = {Action-sound latency and the perceived quality of digital musical instruments: Comparing professional percussionists and amateur musicians},
	volume = {36},
	year = {2018}}

@article{Wessel_Wright_2002,
	author = {Wessel, David and Wright, Matthew},
	date-added = {2023-01-23 12:22:58 +0200},
	date-modified = {2023-01-23 12:23:13 +0200},
	journal = {Computer music journal},
	number = {3},
	pages = {11--22},
	publisher = {JSTOR},
	title = {Problems and prospects for intimate musical control of computers},
	volume = {26},
	year = {2002}}

@article{Moore_1988,
	author = {Moore, F Richard},
	date-added = {2023-01-23 12:12:07 +0200},
	date-modified = {2023-01-23 12:12:26 +0200},
	journal = {Computer music journal},
	number = {1},
	pages = {19--28},
	publisher = {JSTOR},
	title = {The dysfunctions of MIDI},
	volume = {12},
	year = {1988}}

@inproceedings{Tardieu_etal_2009,
	abstract = {An interactive loop between motion recognition and motion generation is a fundamental mechanism for humans and humanoid robots. We have been developing an intelligent framework for motion recognition and generation based on symbolizing motion primitives. The motion primitives are encoded into Hidden Markov Models (HMMs), which we call ``motion symbols''. However, to determine the motion primitives to use as training data for the HMMs, this framework requires a manual segmentation of human motions. Essentially, a humanoid robot is expected to participate in daily life and must learn many motion symbols to adapt to various situations. For this use, manual segmentation is cumbersome and impractical for humanoid robots. In this study, we propose a novel approach to segmentation, the Real-time Unsupervised Segmentation (RUS) method, which comprises three phases. In the first phase, short human movements are encoded into feature HMMs. Seamless human motion can be converted to a sequence of these feature HMMs. In the second phase, the causality between the feature HMMs is extracted. The causality data make it possible to predict movement from observation. In the third phase, movements having a large prediction uncertainty are designated as the boundaries of motion primitives. In this way, human whole-body motion can be segmented into a sequence of motion primitives. This paper also describes an application of RUS to AUtonomous Symbolization of motion primitives (AUS). Each derived motion primitive is classified into an HMM for a motion symbol, and parameters of the HMMs are optimized by using the motion primitives as training data in competitive learning. The HMMs are gradually optimized in such a way that the HMMs can abstract similar motion primitives. We tested the RUS and AUS frameworks on captured human whole-body motions and demonstrated the validity of the proposed framework.},
	author = {D. Tardieu and R. Chessini and J. Dubois and S. Dupont and S. Hidot and B. Mazzarino and A. Moinet and X. Siebert and G. Varni and A. Visentin},
	booktitle = {5th International Summer Workshop on Multimodal Interfaces eNTERFACE'09},
	date-added = {2021-11-01 19:45:39 +0200},
	date-modified = {2023-01-23 13:10:58 +0200},
	editor = {Camurri, Antonio and Mancini, Maurizio and Volpe , Gualtiero},
	issn = {13-978-88-901344-7-0},
	pages = {35 - 40},
	publisher = {University of Genova},
	title = {Video Navigation Tool: Application to browsing a database of dancers' performances},
	url = {https://www.academia.edu/download/29300642/10.1.1.172.3219.pdf#page=45},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1016/j.robot.2015.09.021}}

@article{Takano_Nakamura_2016,
	abstract = {An interactive loop between motion recognition and motion generation is a fundamental mechanism for humans and humanoid robots. We have been developing an intelligent framework for motion recognition and generation based on symbolizing motion primitives. The motion primitives are encoded into Hidden Markov Models (HMMs), which we call ``motion symbols''. However, to determine the motion primitives to use as training data for the HMMs, this framework requires a manual segmentation of human motions. Essentially, a humanoid robot is expected to participate in daily life and must learn many motion symbols to adapt to various situations. For this use, manual segmentation is cumbersome and impractical for humanoid robots. In this study, we propose a novel approach to segmentation, the Real-time Unsupervised Segmentation (RUS) method, which comprises three phases. In the first phase, short human movements are encoded into feature HMMs. Seamless human motion can be converted to a sequence of these feature HMMs. In the second phase, the causality between the feature HMMs is extracted. The causality data make it possible to predict movement from observation. In the third phase, movements having a large prediction uncertainty are designated as the boundaries of motion primitives. In this way, human whole-body motion can be segmented into a sequence of motion primitives. This paper also describes an application of RUS to AUtonomous Symbolization of motion primitives (AUS). Each derived motion primitive is classified into an HMM for a motion symbol, and parameters of the HMMs are optimized by using the motion primitives as training data in competitive learning. The HMMs are gradually optimized in such a way that the HMMs can abstract similar motion primitives. We tested the RUS and AUS frameworks on captured human whole-body motions and demonstrated the validity of the proposed framework.},
	author = {Wataru Takano and Yoshihiko Nakamura},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S092188901500216X},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.robot.2015.09.021},
	date-added = {2021-11-01 19:37:49 +0200},
	date-modified = {2021-11-01 19:37:49 +0200},
	issn = {0921-8890},
	journal = {Robotics and Autonomous Systems},
	keywords = {Motion segmentation, Motion primitive, Competitive learning},
	pages = {260 - 272},
	title = {Real-time Unsupervised Segmentation of human whole-body motion and its application to humanoid robot acquisition of motion symbols},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S092188901500216X},
	volume = {75},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1016/j.robot.2015.09.021}}

@misc{Schatti_2007,
	author = {Sch{\"a}tti, Georg},
	date-added = {2021-11-01 12:48:38 +0200},
	date-modified = {2021-11-01 13:11:18 +0200},
	school = {Institute for Computer Systems, Programming Languages and Runtime Systems Group ETH Zurich},
	title = {Real-Time Audio Feature Analysis for Decklight3},
	url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.7916&rep=rep1&type=pdf},
	year = {2007},
	bdsk-url-1 = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.7916&rep=rep1&type=pdf}}

@article{Cornacchia_et_al_2017,
	abstract = {Activity detection and classification are very important for autonomous monitoring of humans for applications, including assistive living, rehabilitation, and surveillance. Wearable sensors have found wide-spread use in recent years due to their ever-decreasing cost, ease of deployment and use, and ability to provide continuous monitoring as opposed to sensors installed at fixed locations. Since many smart phones are now equipped with a variety of sensors, such as accelerometer, gyroscope, and camera, it has become more feasible to develop activity monitoring algorithms employing one or more of these sensors with increased accessibility. We provide a complete and comprehensive survey on activity classification with wearable sensors, covering a variety of sensing modalities, including accelerometer, gyroscope, pressure sensors, and camera- and depth-based systems. We discuss differences in activity types tackled by this breadth of sensing modalities. For example, accelerometer, gyroscope, and magnetometer systems have a history of addressing whole body motion or global type activities, whereas camera systems provide the context necessary to classify local interactions, or interactions of individuals with objects. We also found that these single sensing modalities laid the foundation for hybrid works that tackle a mix of global and local interaction-type activities. In addition to the type of sensors and type of activities classified, we provide details on each wearable system that include on-body sensor location, employed learning approach, and extent of experimental setup. We further discuss where the processing is performed, i.e., local versus remote processing, for different systems. This is one of the first surveys to provide such breadth of coverage across different wearable sensor systems for activity classification.},
	author = {M. {Cornacchia} and K. {Ozcan} and Y. {Zheng} and S. {Velipasalar}},
	date-added = {2020-05-18 13:01:25 +0000},
	date-modified = {2020-05-18 13:01:57 +0000},
	doi = {10.1109/JSEN.2016.2628346},
	issn = {1558-1748},
	journal = {IEEE Sensors Journal},
	keywords = {accelerometers;body sensor networks;cameras;gyroscopes;pressure measurement;pressure sensors;activity detection;activity classification;wearable sensor system;humans autonomous monitoring;assistive living;rehabilitation;surveillance;smart phone;accelerometer;gyroscope;activity monitoring algorithm;pressure sensor;depth-based system;magnetometer system;camera system;single sensing modality;interaction-type activity;on-body sensor location;Wearable sensors;Sensor systems;Accelerometers;Cameras;Monitoring;Magnetic sensors;Wearable;sensors;survey;activity detection;activity classification;monitoring},
	month = {Jan},
	number = {2},
	pages = {386-403},
	title = {A Survey on Activity Detection and Classification Using Wearable Sensors},
	volume = {17},
	year = {2017},
	bdsk-url-1 = {http://dx.doi.org/10.1109/JSEN.2016.2628346}}

@misc{Petzold_ca1725,
	author = {Petzold, Christian},
	date-added = {2020-05-17 02:25:56 +0000},
	date-modified = {2020-05-17 05:53:03 +0000},
	howpublished = {The Anna Magdalena Bach Notebook, Anh. 114.},
	title = {Minuet in {G} Major},
	year = {ca. 1725}}

@misc{Otondo_2008,
	author = {Otondo, Felipe},
	date-added = {2020-05-17 02:18:41 +0000},
	date-modified = {2020-05-17 05:16:03 +0000},
	howpublished = {in: Tutuguri. Sargasso.},
	title = {Ciguri},
	year = {2008}}

@article{Liu_et_al_2013,
	abstract = {The objective of change-point detection is to discover abrupt property changes lying behind time-series data. In this paper, we present a novel statistical change-point detection algorithm based on non-parametric divergence estimation between time-series samples from two retrospective segments. Our method uses the relative Pearson divergence as a divergence measure, and it is accurately and efficiently estimated by a method of direct density-ratio estimation. Through experiments on artificial and real-world datasets including human-activity sensing, speech, and Twitter messages, we demonstrate the usefulness of the proposed method.},
	author = {Song Liu and Makoto Yamada and Nigel Collier and Masashi Sugiyama},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S0893608013000270},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.neunet.2013.01.012},
	date-added = {2020-05-07 05:17:02 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Change-point detection, Distribution comparison, Relative density-ratio estimation, Kernel methods, Time-series data},
	pages = {72 - 83},
	title = {Change-point detection in time-series data by relative density-ratio estimation},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S0893608013000270},
	volume = {43},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1016/j.neunet.2013.01.012}}

@article{Fathy_Barnaghi_Tafazolli_2019,
	abstract = {There has been a keen interest in detecting abrupt sequential changes in streaming data obtained from sensors in wireless sensor networks for Internet of Things applications, such as fire/fault detection, activity recognition, and environmental monitoring. Such applications require (near) online detection of instantaneous changes. This paper proposes an online, adaptive filtering-based change detection (OFCD) algorithm. Our method is based on a convex combination of two decoupled least mean square windowed filters with differing sizes. Both filters are applied independently on data streams obtained from sensor nodes such that their convex combination parameter is employed as an indicator of abrupt changes in mean values. An extension of our method (OFCD) based on a cooperative scheme between multiple sensors (COFCD) is also presented. It provides an enhancement of both convergence and steady-state accuracy of the convex weight parameter. Our conducted experiments show that our approach can be applied in distributed networks in an online fashion. It also provides better performance and less complexity compared with the state-of-the-art on both of single and multiple sensors.},
	author = {Y. {Fathy} and P. {Barnaghi} and R. {Tafazolli}},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/JSYST.2018.2876461},
	date-added = {2020-05-07 04:07:50 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {1937-9234},
	journal = {IEEE Systems Journal},
	keywords = {adaptive filters;fault diagnosis;unsupervised learning;wireless sensor networks;convex combination parameter;multiple sensors;convex weight parameter;sensory data;abrupt sequential changes;wireless sensor networks;instantaneous changes;decoupled least mean square;data streams;sensor nodes;online adaptive filtering-based change detection algorithm;COFCD;Monitoring;Wireless sensor networks;Change detection algorithms;Delays;Microsoft Windows;Sensor systems;Cooperative (diffusion-based) strategy;mean change detection;multi-sensory data;streaming data},
	month = {Sep.},
	number = {3},
	pages = {2688-2699},
	title = {An Online Adaptive Algorithm for Change Detection in Streaming Sensory Data},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {http://dx.doi.org/10.1109/JSYST.2018.2876461}}

@article{Patterson_et_al_2017,
	abstract = {The accurate detection of changes has the potential to form a fundamental component of systems which autonomously solicit user interaction based on transitions within an input stream, for example, electrocardiogram data or accelerometry obtained from a mobile device. This solicited interaction may be utilized for diverse scenarios such as responding to changes in a patient's vital signs within a medical domain or requesting user activity labels for generating real-world labelled datasets. Within this paper, we extend our previous work on the Multivariate Online Change detection Algorithm subsequently exploring the utility of incorporating the Benjamini Hochberg method of correcting for multiple comparisons. Furthermore, we evaluate our approach against similarly light-weight Multivariate Exponentially Weighted Moving Average and Cumulative Sum based techniques. Results are presented based on manually labelled change points in accelerometry data captured using 10 participants. Each participant performed nine distinct activities for a total period of 35 minutes. The results subsequently demonstrate the practical potential of our approach from both accuracy and computational perspectives.},
	author = {T. {Patterson} and N. {Khan} and S. {McClean} and C. {Nugent} and S. {Zhang} and I. {Cleland} and Q. {Ni}},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TMC.2016.2640959},
	date-added = {2020-05-06 18:35:33 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {1558-0660},
	journal = {IEEE Transactions on Mobile Computing},
	keywords = {human computer interaction;moving average processes;multivariate exponentially weighted moving average;cumulative sum;Benjamini Hochberg method;multivariate online change detection;user interaction;user engagement;timely solicitation;sensor-based change detection;Change detection algorithms;Detection algorithms;Time series analysis;Signal processing algorithms;Mobile computing;Heuristic algorithms;Computational modeling;Multivariate change detection;online change detection;soliciting user interaction},
	month = {Oct},
	number = {10},
	pages = {2889-2900},
	title = {Sensor-Based Change Detection for Timely Solicitation of User Engagement},
	volume = {16},
	year = {2017},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TMC.2016.2640959}}

@article{Aminikhanghahi_Cook_2017,
	abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
	author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
	bdsk-url_opt-1 = {https://doi.org/10.1007/s10115-016-0987-z},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/s10115-016-0987-z},
	da = {2017/05/01},
	date-added = {2020-05-05 07:04:57 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	id = {Aminikhanghahi2017},
	isbn = {0219-3116},
	journal = {Knowledge and Information Systems},
	number = {2},
	pages = {339--367},
	title = {A survey of methods for time series change point detection},
	ty = {JOUR},
	url_opt = {https://doi.org/10.1007/s10115-016-0987-z},
	volume = {51},
	year = {2017},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s10115-016-0987-z}}

@article{Zameni_et_al_2020,
	abstract = {A critical problem in time series analysis is change point detection, which identifies the times when the underlying distribution of a time series abruptly changes. However, several shortcomings limit the use of some existing techniques in real-world applications. First, several change point detection techniques are offline methods, where the whole time series needs to be stored before change point detection can be performed. These methods are not applicable to streaming time series. Second, most techniques assume that the time series is low-dimensional and hence have problems handling high-dimensional time series, where not all dimensions may cause the change. Finally, most methods require user-defined parameters that need to be chosen based on the observed data, which limits their applicability to new unseen data. To address these issues, we propose an Information Gain-based method that does not require prior distributional knowledge for detecting change points and handles high-dimensional time series. The advantages of our proposed method compared to the state-of-the-art algorithms are demonstrated from theoretical basis, as well as via experiments on four synthetic and three real-world human activity datasets.},
	author = {Zameni, Masoomeh and Sadri, Amin and Ghafoori, Zahra and Moshtaghi, Masud and Salim, Flora D. and Leckie, Christopher and Ramamohanarao, Kotagiri},
	bdsk-url_opt-1 = {https://doi.org/10.1007/s10115-019-01366-x},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/s10115-019-01366-x},
	da = {2020/02/01},
	date-added = {2020-05-04 15:59:17 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	id = {Zameni2020},
	isbn = {0219-3116},
	journal = {Knowledge and Information Systems},
	number = {2},
	pages = {719--750},
	title = {Unsupervised online change point detection in high-dimensional time series},
	ty = {JOUR},
	url_opt = {https://doi.org/10.1007/s10115-019-01366-x},
	volume = {62},
	year = {2020},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s10115-019-01366-x}}

@article{Reily_et_al_2018,
	author = {Reily, Brian and Han, Fei and Parker, Lynne E and Zhang, Hao},
	date-added = {2019-04-13 12:19:26 +0000},
	date-modified = {2019-04-13 12:20:50 +0000},
	journal = {Autonomous Robots},
	number = {6},
	pages = {1281--1298},
	publisher = {Springer},
	title = {Skeleton-based bio-inspired human activity prediction for real-time human--robot interaction},
	volume = {42},
	year = {2018}}

@inproceedings{Buetepage_Kjellstroem_Kragic_2018,
	author = {B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Kragic, Danica},
	booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
	date-added = {2019-04-13 12:18:29 +0000},
	date-modified = {2019-04-13 12:20:26 +0000},
	organization = {IEEE},
	pages = {1--9},
	title = {Anticipating many futures: Online human motion prediction and generation for human-robot interaction},
	year = {2018}}

@misc{Gibb_Gibb_Gibb_1977,
	author = {B. Gibb and R. Gibb and M. Gibb},
	date-added = {2019-04-09 15:46:04 +0000},
	date-modified = {2019-04-09 15:47:24 +0000},
	howpublished = {On Saturday Night Fever, The Original Motion Picture Soundtrack. Germany: RSO},
	title = {Stayin' Alive},
	year = {1977}}

@article{Burger_et_al_2013,
	author = {Burger, Birgitta and Saarikallio, Suvi and Luck, Geoff and Thompson, Marc R and Toiviainen, Petri},
	date-added = {2019-04-09 15:43:59 +0000},
	date-modified = {2019-04-09 15:44:27 +0000},
	journal = {Music Perception: An Interdisciplinary Journal},
	number = {5},
	pages = {517--533},
	publisher = {University of California Press Journals},
	title = {Relationships between perceived emotions in music and music-induced movement},
	volume = {30},
	year = {2013}}

@misc{Little_Sizemore_Shay_nd,
	author = {G. Little and A. Sizemore and L. Shay},
	date-added = {2019-04-09 15:24:27 +0000},
	date-modified = {2019-04-09 16:05:32 +0000},
	howpublished = {Recorded by Steven Mitchell and Gordon Webster band. On Live in Rochester (CD). Harro East Ballroom, Rochester, N.Y.: Gordon Webster Swings (18-19 November, 2012)},
	title = {I Like Pie, I like Cake},
	year = {n.d.}}

@inproceedings{Nagano_et_al_2018,
	author = {Nagano, Masatoshi and Nakamura, Tomoaki and Nagai, Takayuki and Mochihashi, Daichi and Kobayashi, Ichiro and Kaneko, Masahide},
	booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	date-added = {2019-04-09 15:06:56 +0000},
	date-modified = {2019-04-09 16:00:03 +0000},
	organization = {IEEE},
	pages = {4067--4074},
	title = {Sequence Pattern Extraction by Segmenting Time Series Data Using GP-HSMM with Hierarchical Dirichlet Process},
	year = {2018}}

@inproceedings{Matsubara_Sakurai_Faloustos_2014,
	author = {Matsubara, Yasuko and Sakurai, Yasushi and Faloutsos, Christos},
	booktitle = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data},
	date-added = {2019-04-09 15:05:12 +0000},
	date-modified = {2019-04-09 15:05:48 +0000},
	organization = {ACM},
	pages = {193-204},
	title = {Autoplait: Automatic mining of co-evolving time sequences},
	year = {2014}}

@article{Nakamura_et_al_2017,
	author = {Nakamura, Tomoaki and Nagai, Takayuki and Mochihashi, Daichi and Kobayashi, Ichiro and Asoh, Hideki and Kaneko, Masahide},
	date-added = {2019-04-09 14:01:31 +0000},
	date-modified = {2019-04-09 15:10:51 +0000},
	journal = {Frontiers in neurorobotics},
	number = {67},
	publisher = {Frontiers},
	title = {Segmenting continuous motions with hidden semi-markov models and gaussian processes},
	volume = {11},
	year = {2017}}

@electronic{CMU,
	bdsk-url_opt-1 = {http://mocap.cs.cmu.edu/},
	date-added = {2019-04-09 13:33:27 +0000},
	date-modified = {2019-04-13 13:11:42 +0000},
	month = {October},
	title = {{Carnegie Mellon University Graphics Lab} motion capture database},
	url_opt = {http://mocap.cs.cmu.edu/},
	year = {2018}}

@inproceedings{Burger_Toiviainen_2013,
	address = {Stockholm, Sweden},
	author = {Burger, Birgitta and Toiviainen, Petri},
	booktitle = {Proceedings of the 10th Sound and Music Computing Conference},
	date-added = {2019-04-09 13:14:14 +0000},
	date-modified = {2019-04-09 13:14:30 +0000},
	editor = {Bresin, Roberto},
	pages = {172--178},
	publisher = {KTH Royal Institute of Technology},
	title = {{MoCap Toolbox -- A Matlab toolbox for computational analysis of movement data}},
	year = {2013}}

@inproceedings{Mendoza_Thompson_2017,
	author = {Mendoza, Juan Ignacio and Thompson, Marc Richard},
	booktitle = {Conference proceedings of the 25th Anniversary Edition of the European Society for the Cognitive Sciences of Music.},
	date-added = {2019-04-05 15:50:00 +0000},
	date-modified = {2020-05-17 05:57:42 +0000},
	editor = {Van Dyck, E.},
	organization = {Ghent University},
	pages = {128-133},
	title = {Modelling Perceived Segmentation of Bodily Gestures Induced by Music},
	year = {2017}}

@mastersthesis{Mendoza_2014,
	author = {Mendoza, Juan Ignacio},
	date-added = {2019-04-05 15:46:51 +0000},
	date-modified = {2019-04-13 04:10:38 +0000},
	school = {University of Jyv{\"a}skyl{\"a}},
	title = {Self-report measurement of segmentation, mimesis and perceived emotions in acousmatic electroacoustic music},
	year = {2014}}

@article{Junejo_et_al_2011,
	abstract = {This paper addresses recognition of human actions under view changes. We explore self-similarities of action sequences over time and observe the striking stability of such measures across views. Building upon this key observation, we develop an action descriptor that captures the structure of temporal similarities and dissimilarities within an action sequence. Despite this temporal self-similarity descriptor not being strictly view-invariant, we provide intuition and experimental validation demonstrating its high stability under view changes. Self-similarity descriptors are also shown to be stable under performance variations within a class of actions when individual speed fluctuations are ignored. If required, such fluctuations between two different instances of the same action class can be explicitly recovered with dynamic time warping, as will be demonstrated, to achieve cross-view action synchronization. More central to the current work, temporal ordering of local self-similarity descriptors can simply be ignored within a bag-of-features type of approach. Sufficient action discrimination is still retained in this way to build a view-independent action recognition system. Interestingly, self-similarities computed from different image features possess similar properties and can be used in a complementary fashion. Our method is simple and requires neither structure recovery nor multiview correspondence estimation. Instead, it relies on weak geometric properties and combines them with machine learning for efficient cross-view action recognition. The method is validated on three public data sets. It has similar or superior performance compared to related methods and it performs well even in extreme conditions, such as when recognizing actions from top views while using side views only for training.},
	author = {I. N. Junejo and E. Dexter and I. Laptev and P. Perez},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TPAMI.2010.68},
	booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	date-added = {2019-04-05 15:43:55 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	journal1 = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {computational geometry; feature extraction; image sequences; learning (artificial intelligence); view independent action recognition; action sequence; temporal self similarity; action discrimination; image feature; striking stability; action descriptor; dynamic time warping; cross view action synchronization; temporal ordering; bag of features type approach; multiview correspondence estimation; weak geometric property; machine learning; public data set; Humans; Cameras; Hidden Markov models; Stability; Fluctuations; Machine learning; Shape; Support vector machines; Time measurement; Buildings; Human action recognition; human action synchronization; view invariance; temporal self-similarities; local temporal descriptors.; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Movement; Pattern Recognition, Automated},
	number = {1},
	pages = {172--185},
	title = {View-Independent Action Recognition from Temporal Self-Similarities},
	ty = {JOUR},
	vo = {33},
	volume = {33},
	year = {2011},
	year1 = {Jan. 2011},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TPAMI.2010.68}}

@inproceedings{Foote_Cooper_2003,
	author = {Jonathan Foote and Matthew L. Cooper},
	booktitle = {Proc. SPIE, Storage and Retrieval for Media Databases},
	date-added = {2019-04-05 15:23:58 +0000},
	date-modified = {2019-04-05 15:42:06 +0000},
	month = {January},
	organization = {International Society for Optics and Photonics},
	pages = {167-175},
	title = {Media segmentation using self-similarity decomposition},
	volume = {5021},
	year = {2003}}

@inproceedings{Foote_2000,
	abstract = {The paper describes methods for automatically locating points of significant change in music or audio, by analyzing local self-similarity. This method can find individual note boundaries or even natural segment boundaries such as verse/chorus or speech/music transitions, even in the absence of cues such as silence. This approach uses the signal to model itself, and thus does not rely on particular acoustic cues nor requires training. We present a wide variety of applications, including indexing, segmenting, and beat tracking of music and audio. The method works well on a wide variety of audio sources.},
	author = {Jonathan Foote},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/ICME.2000.869637},
	booktitle = {2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings.},
	date-added = {2019-04-05 15:07:33 +0000},
	date-modified = {2021-12-29 14:58:19 +0200},
	journal = {2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)},
	journal1 = {2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)},
	keywords = {audio signal processing; music; fractals; signal sources; multimedia systems; indexing; automatic audio segmentation; audio novelty measure; local self-similarity analysis; individual note boundaries; natural segment boundaries; verse/chorus; speech/music transitions; acoustic cues; indexing; segmenting; beat tracking; music; audio sources; Speech; Laboratories; Indexing; Acoustic measurements; Computer applications; Time measurement; Spectral analysis; Frequency domain analysis; Fast Fourier transforms; Mel frequency cepstral coefficient},
	pages = {452--455 vol.1},
	title = {Automatic audio segmentation using a measure of audio novelty},
	ty = {CONF},
	vo = {1},
	volume = {1},
	year = {2000},
	year1 = {30 July-2 Aug. 2000},
	bdsk-url-1 = {http://dx.doi.org/10.1109/ICME.2000.869637}}

@article{Markou_Singh_2003,
	abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.},
	author = {Markou, Markos and Singh, Sameer},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S0165168403002020},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.sigpro.2003.07.018},
	da = {2003/12/01/},
	date-added = {2019-04-05 15:06:17 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {0165-1684},
	journal = {Signal Processing},
	keywords = {Novelty detection review; Statistical approaches; Gaussian mixture models; Hidden Markov models; KNN; Parzen density estimation; String matching; Clustering},
	number = {12},
	pages = {2481--2497},
	title = {Novelty detection: a review---part 1: statistical approaches},
	ty = {JOUR},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S0165168403002020},
	volume = {83},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1016/j.sigpro.2003.07.018}}

@article{Endres_et_al_2011,
	acmid = {2010326},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Endres, Dominik and Christensen, Andrea and Omlor, Lars and Giese, Martin A.},
	bdsk-url_opt-1 = {http://doi.acm.org/10.1145/2010325.2010326},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1145/2010325.2010326},
	date-added = {2019-04-05 15:04:53 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {1544-3558},
	issue_date = {August 2011},
	journal = {ACM Trans. Appl. Percept.},
	keywords = {Bayesian methods, Motion capture, action segmentation, unsupervised learning},
	month = aug,
	number = {3},
	numpages = {12},
	pages = {16:1--16:12},
	publisher = {ACM},
	title = {Emulating Human Observers with Bayesian Binning: Segmentation of Action Streams},
	url_opt = {http://doi.acm.org/10.1145/2010325.2010326},
	volume = {8},
	year = {2011},
	bdsk-url-1 = {http://dx.doi.org/10.1145/2010325.2010326}}

@article{Zhou_De_la_Torre_Hodgins_2013,
	abstract = {Temporal segmentation of human motion into plausible motion primitives is central to understanding and building computational models of human motion. Several issues contribute to the challenge of discovering motion primitives: the exponential nature of all possible movement combinations, the variability in the temporal scale of human actions, and the complexity of representing articulated motion. We pose the problem of learning motion primitives as one of temporal clustering, and derive an unsupervised hierarchical bottom-up framework called hierarchical aligned cluster analysis (HACA). HACA finds a partition of a given multidimensional time series into m disjoint segments such that each segment belongs to one of k clusters. HACA combines kernel k-means with the generalized dynamic time alignment kernel to cluster time series data. Moreover, it provides a natural framework to find a low-dimensional embedding for time series. HACA is efficiently optimized with a coordinate descent strategy and dynamic programming. Experimental results on motion capture and video data demonstrate the effectiveness of HACA for segmenting complex motions and as a visualization tool. We also compare the performance of HACA to state-of-the-art algorithms for temporal clustering on data of a honey bee dance. The HACA code is available online.},
	author = {F. Zhou and F. D. l. Torre and J. K. Hodgins},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TPAMI.2012.137},
	booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	date-added = {2019-04-05 15:03:09 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	journal1 = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {dynamic programming; image motion analysis; image representation; image segmentation; pattern clustering; time series; hierarchical aligned cluster analysis; temporal clustering; human motion; temporal segmentation; plausible motion; motion primitives discovery; exponential nature; temporal scale; articulated motion representation; HACA; multidimensional time series; m disjoint segments; k clusters; dynamic programming; Kernel; Time series analysis; Humans; Motion segmentation; Clustering algorithms; Heuristic algorithms; Legged locomotion; Temporal segmentation; time series clustering; time series visualization; human motion analysis; kernel k-means; spectral clustering; dynamic programming; Algorithms; Animals; Bees; Behavior, Animal; Cluster Analysis; Computer Simulation; Humans; Image Processing, Computer-Assisted; Locomotion; Spatio-Temporal Analysis; Video Recording},
	number = {3},
	pages = {582--596},
	title = {Hierarchical Aligned Cluster Analysis for Temporal Clustering of Human Motion},
	ty = {JOUR},
	vo = {35},
	volume = {35},
	year = {2013},
	year1 = {March 2013},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TPAMI.2012.137}}

@article{Gharghabi_et_al_2019,
	abstract = {Unsupervised semantic segmentation in the time series domain is a much studied problem due to its potential to detect unexpected regularities and regimes in poorly understood data. However, the current techniques have several shortcomings, which have limited the adoption of time series semantic segmentation beyond academic settings for four primary reasons. First, most methods require setting/learning many parameters and thus may have problems generalizing to novel situations. Second, most methods implicitly assume that all the data is segmentable and have difficulty when that assumption is unwarranted. Thirdly, many algorithms are only defined for the single dimensional case, despite the ubiquity of multi-dimensional data. Finally, most research efforts have been confined to the batch case, but online segmentation is clearly more useful and actionable. To address these issues, we present a multi-dimensional algorithm, which is domain agnostic, has only one, easily-determined parameter, and can handle data streaming at a high rate. In this context, we test the algorithm on the largest and most diverse collection of time series datasets ever considered for this task and demonstrate the algorithm's superiority over current solutions.},
	author = {Gharghabi, Shaghayegh and Yeh, Chin-Chia Michael and Ding, Yifei and Ding, Wei and Hibbing, Paul and LaMunion, Samuel and Kaplan, Andrew and Crouter, Scott E. and Keogh, Eamonn},
	bdsk-url_opt-1 = {https://doi.org/10.1007/s10618-018-0589-3},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/s10618-018-0589-3},
	date-added = {2019-04-05 14:54:02 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	day = {01},
	issn = {1573-756X},
	journal = {Data Mining and Knowledge Discovery},
	month = {Jan},
	number = {1},
	pages = {96--130},
	title = {Domain agnostic online semantic segmentation for multi-dimensional time series},
	url_opt = {https://doi.org/10.1007/s10618-018-0589-3},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s10618-018-0589-3}}

@article{Xia_et_al_2018,
	abstract = {Studies on human motion have attracted a lot of attentions. Human motion capture data, which much more precisely records human motion than videos do, has been widely used in many areas. Motion segmentation is an indispensable step for many related applications, but current segmentation methods for motion capture data do not effectively model some important characteristics of motion capture data, such as Riemannian manifold structure and containing non-Gaussian noise. In this paper, we convert the segmentation of motion capture data into a temporal subspace clustering problem. Under the framework of sparse subspace clustering, we propose to use the geodesic exponential kernel to model the Riemannian manifold structure, use correntropy to measure the reconstruction error, use the triangle constraint to guarantee temporal continuity in each cluster and use multi-view reconstruction to extract the relations between different joints. Therefore, exploiting some special characteristics of motion capture data, we propose a new segmentation method, which is robust to non-Gaussian noise, since correntropy is a localized similarity measure. We also develop an efficient optimization algorithm based on block coordinate descent method to solve the proposed model. Our optimization algorithm has a linear complexity while sparse subspace clustering is originally a quadratic problem. Extensive experiment results both on simulated noisy data set and real noisy data set demonstrate the advantage of the proposed method.},
	author = {G. Xia and H. Sun and L. Feng and G. Zhang and Y. Liu},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TIP.2017.2738562},
	booktitle = {IEEE Transactions on Image Processing},
	date-added = {2019-04-05 14:51:15 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {1057-7149},
	journal = {IEEE Transactions on Image Processing},
	journal1 = {IEEE Transactions on Image Processing},
	keywords = {differential geometry; Gaussian noise; gradient methods; image motion analysis; image reconstruction; image segmentation; optimisation; pattern clustering; Riemannian manifold structure; temporal subspace clustering problem; human motion segmentation; robust kernel sparse subspace clustering; nonGaussian noise; geodesic exponential kernel; correntropy; reconstruction error measurement; triangle constraint; temporal continuity; multiview reconstruction; optimization algorithm; linear complexity; quadratic problem; simulated noisy data set; real noisy data set; human motion capture data; Motion segmentation; Manifolds; Computer vision; Data models; Kernel; Robustness; Motion capture data; robust segmentation; Riemannian manifold; correntropy; subspace clustering},
	number = {1},
	pages = {135--150},
	title = {Human Motion Segmentation via Robust Kernel Sparse Subspace Clustering},
	ty = {JOUR},
	vo = {27},
	volume = {27},
	year = {2018},
	year1 = {Jan. 2018},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2017.2738562}}

@article{Gong_Medioni_Zhao_2014,
	abstract = {We address the problem of structure learning of human motion in order to recognize actions from a continuous monocular motion sequence of an arbitrary person from an arbitrary viewpoint. Human motion sequences are represented by multivariate time series in the joint-trajectories space. Under this structured time series framework, we first propose Kernelized Temporal Cut (KTC), an extension of previous works on change-point detection by incorporating Hilbert space embedding of distributions, to handle the nonparametric and high dimensionality issues of human motions. Experimental results demonstrate the effectiveness of our approach, which yields realtime segmentation, and produces high action segmentation accuracy. Second, a spatio-temporal manifold framework is proposed to model the latent structure of time series data. Then an efficient spatio-temporal alignment algorithm Dynamic Manifold Warping (DMW) is proposed for multivariate time series to calculate motion similarity between action sequences (segments). Furthermore, by combining the temporal segmentation algorithm and the alignment algorithm, online human action recognition can be performed by associating a few labeled examples from motion capture data. The results on human motion capture data and 3D depth sensor data demonstrate the effectiveness of the proposed approach in automatically segmenting and recognizing motion sequences, and its ability to handle noisy and partially occluded data, in the transfer learning module.},
	author = {D. Gong and G. Medioni and X. Zhao},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TPAMI.2013.244},
	booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	date-added = {2019-04-05 14:48:52 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	journal1 = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Hilbert spaces; image motion analysis; image recognition; image segmentation; image sequences; learning (artificial intelligence); time series; structured time series analysis; human action segmentation; human action recognition; structure learning; continuous monocular motion sequence; multivariate time series; joint-trajectories space; kernelized temporal cut; change-point detection; Hilbert space; realtime segmentation; high action segmentation; spatio-temporal manifold framework; spatio-temporal alignment algorithm; dynamic manifold warping; action sequences; human motion capture data; 3D depth sensor data; transfer learning module; Motion segmentation; Three-dimensional displays; Kernel; Time series analysis; Manifolds; Heuristic algorithms; Hidden Markov models; Machine learning; Computer vision; Multivariate time series; action recognition; online temporal segmentation; saptio-temporal alignment; transfer learning; Actigraphy; Algorithms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Motor Activity; Movement; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Video Recording; Whole Body Imaging},
	number = {7},
	pages = {1414--1427},
	title = {Structured Time Series Analysis for Human Action Segmentation and Recognition},
	ty = {JOUR},
	vo = {36},
	volume = {36},
	year = {2014},
	year1 = {July 2014},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TPAMI.2013.244}}

@article{Bargi_Da_Xu_Piccardi_2018,
	abstract = {Recent years have witnessed an increasing need for the automated classification of sequential data, such as activities of daily living, social media interactions, financial series, and others. With the continuous flow of new data, it is critical to classify the observations on-the-fly and without being limited by a predetermined number of classes. In addition, a model should be able to update its parameters in response to a possible evolution in the distributions of the classes. This compelling problem, however, does not seem to have been adequately addressed in the literature, since most studies focus on offline classification over predefined class sets. In this paper, we present a principled solution for this problem based on an adaptive online system leveraging Markov switching models and hierarchical Dirichlet process priors. This adaptive online approach is capable of classifying the sequential data over an unlimited number of classes while meeting the memory and delay constraints typical of streaming contexts. In this paper, we introduce an adaptive {\^a}{\oe}learning rate{\^a}that is responsible for balancing the extent to which the model retains its previous parameters or adapts to new observations. Experimental results on stationary and evolving synthetic data and two video data sets, TUM Assistive Kitchen and collated Weizmann, show a remarkable performance in terms of segmentation and classification, particularly for sequences from evolutionary distributions and/or those containing previously unseen classes.},
	author = {A. Bargi and R. Y. D. Xu and M. Piccardi},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TNNLS.2017.2742058},
	booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
	date-added = {2019-04-05 14:47:09 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {2162-237X},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	journal1 = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Hidden Markov models; Adaptation models; Data models; Bayes methods; Markov processes; Switches; Training; Activity segmentation and classification; adaptive learning rate; hidden Markov model; hierarchical Dirichlet process; online learning},
	number = {9},
	pages = {3953--3968},
	title = {AdOn HDP-HMM: An Adaptive Online Model for Segmentation and Classification of Sequential Data},
	ty = {JOUR},
	vo = {29},
	volume = {29},
	year = {2018},
	year1 = {Sept. 2018},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TNNLS.2017.2742058}}

@article{Taniguchi_Hamahata_Iwahashi_2011,
	abstract = { We propose an unsupervised motion segmentation method for enabling a robot to imitate and perform various unit motions by observing unsegmented human motion. Natural unsegmented human motion data contain various types of unit motions, such as 'waving good-bye', 'walking' and 'throwing a ball'. A robot has to segment the data and extract unit motions from the data to imitate the motions. In previous work, an ergodic hidden Markov model (HMM) was used to model unsegmented human motion. However, there are two main problems with the classical use of this model. (i) Setting an appropriate number of hidden states is difficult because how complex the motions contained in the learning data are and how many there are unknown. (ii) We did not have an effective chunking method that could chunk elemental motions into meaningful unit motions without being captured by local minima. To overcome these problems, we developed an unsupervised motion segmentation method for imitation learning using a sticky hierarchical Dirichlet process (HDP)-HMM, a nonparametric Bayesian model, and an unsupervised chunking method based on a Gibbs sampler and the minimal description length (MDL) principle of imitation learning of unsegmented human motion. We developed this chunking method to work with the sticky HDP-HMM and extract unit human motions. We conducted several experiments to evaluate this method. The proposed method could extract unit motions from unsegmented human motion data. The sticky HDP-HMM can be used to model unsegmented human motion more accurately than with a conventional HMM and simultaneously estimate the number of hidden states. We also evaluated the dependency of the HDP-HMM on the hyperparameters of the model. },
	author = {Tadahiro Taniguchi and Keita Hamahata and Naoto Iwahashi},
	bdsk-url_opt-1 = {https://doi.org/10.1163/016918611X594775},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1163/016918611X594775},
	date-added = {2019-04-05 14:45:26 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	eprint_opt = {https://doi.org/10.1163/016918611X594775},
	journal = {Advanced Robotics},
	number = {17},
	pages = {2143-2172},
	publisher = {Taylor &amp; Francis},
	title = {Unsupervised Segmentation of Human Motion Data Using a Sticky Hierarchical Dirichlet Process-Hidden Markov Model and Minimal Description Length-Based Chunking Method for Imitation Learning},
	url_opt = {https://doi.org/10.1163/016918611X594775},
	volume = {25},
	year = {2011},
	bdsk-url-1 = {http://dx.doi.org/10.1163/016918611X594775}}

@article{Kulic_Takano_Nakamura_2009,
	abstract = {This paper describes a novel approach for incremental learning of human motion pattern primitives through online observation of human motion. The observed time series data stream is first stochastically segmented into potential motion primitive segments, based on the assumption that data belonging to the same motion primitive will have the same underlying distribution. The motion segments are then abstracted into a stochastic model representation and automatically clustered and organized. As new motion patterns are observed, they are incrementally grouped together into a tree structure, based on their relative distance in the model space. The tree leaves, which represent the most specialized learned motion primitives, are then passed back to the segmentation algorithm so that as the number of known motion primitives increases, the accuracy of the segmentation can also be improved. The combined algorithm is tested on a sequence of continuous human motion data that are obtained through motion capture, and demonstrates the performance of the proposed approach.},
	author = {D. Kulic and W. Takano and Y. Nakamura},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TRO.2009.2026508},
	booktitle = {IEEE Transactions on Robotics},
	date-added = {2019-04-05 14:41:00 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {1552-3098},
	journal = {IEEE Transactions on Robotics},
	journal1 = {IEEE Transactions on Robotics},
	keywords = {humanoid robots; image motion analysis; image segmentation; intelligent robots; learning (artificial intelligence); pattern clustering; robot vision; stochastic processes; time series; trees (mathematics); online segmentation; incremental learning; human motion pattern primitives; online observation; time series data stream; stochastic segmentation; stochastic model representation; tree structure; clustering algorithm; humanoid robots; Robotics and automation; Clustering algorithms; Human robot interaction; Motion segmentation; Computer vision; Intelligent robots; Societies; Stochastic processes; Tree data structures; Testing; Humanoid robots; incremental learning; learning from observation; motion segmentation and clustering},
	number = {5},
	pages = {1158--1166},
	title = {Online Segmentation and Clustering From Continuous Observation of Whole Body Motions},
	ty = {JOUR},
	vo = {25},
	volume = {25},
	year = {2009},
	year1 = {Oct. 2009},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TRO.2009.2026508}}

@inproceedings{Kohlmorgen_Lemm_2002,
	author = {Jens Kohlmorgen and Steven Lemm},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-04-05 14:35:19 +0000},
	date-modified = {2019-04-05 14:39:26 +0000},
	pages = {793--800},
	publisher = {MIT Press},
	title = {A Dynamic HMM for On-line Segmentation of Sequential Data},
	volume = {14},
	year = {2002}}

@inproceedings{Barbic_et_al_2004,
	acmid = {1006081},
	address = {School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada},
	author = {Barbi\v{c}, Jernej and Safonova, Alla and Pan, Jia-Yu and Faloutsos, Christos and Hodgins, Jessica K. and Pollard, Nancy S.},
	bdsk-url_opt-1 = {http://dl.acm.org/citation.cfm?id=1006058.1006081},
	booktitle = {Proceedings of Graphics Interface 2004},
	date-added = {2019-04-05 14:23:35 +0000},
	date-modified = {2019-04-05 14:23:52 +0000},
	isbn = {1-56881-227-2},
	keywords = {PCA, human motion, motion capture, motion segmentation},
	location = {London, Ontario, Canada},
	numpages = {10},
	pages = {185--194},
	publisher = {Canadian Human-Computer Communications Society},
	series = {GI '04},
	title = {Segmenting Motion Capture Data into Distinct Behaviors},
	url_opt = {http://dl.acm.org/citation.cfm?id=1006058.1006081},
	year = {2004}}

@article{Mueller_Roeder_Clausen_2005,
	acmid = {1073247},
	address = {New York, NY, USA},
	author = {M\"{u}ller, Meinard and R\"{o}der, Tido and Clausen, Michael},
	bdsk-url_opt-1 = {http://doi.acm.org/10.1145/1073204.1073247},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1145/1073204.1073247},
	date-added = {2019-04-05 14:21:45 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0730-0301},
	issue_date = {July 2005},
	journal = {ACM Trans. Graph.},
	keywords = {adaptive segmentation, geometric feature, indexing, motion capture, retrieval time alignment},
	month = jul,
	number = {3},
	numpages = {9},
	pages = {677--685},
	publisher = {ACM},
	title = {Efficient Content-based Retrieval of Motion Capture Data},
	url_opt = {http://doi.acm.org/10.1145/1073204.1073247},
	volume = {24},
	year = {2005},
	bdsk-url-1 = {http://dx.doi.org/10.1145/1073204.1073247}}

@inproceedings{Lv_Nevatia_2006,
	abstract = {Our goal is to automatically segment and recognize basic human actions, such as stand, walk and wave hands, from a sequence of joint positions or pose angles. Such recognition is difficult due to high dimensionality of the data and large spatial and temporal variations in the same action. We decompose the high dimensional 3-D joint space into a set of feature spaces where each feature corresponds to the motion of a single joint or combination of related multiple joints. For each feature, the dynamics of each action class is learned with one HMM. Given a sequence, the observation probability is computed in each HMM and a weak classifier for that feature is formed based on those probabilities. The weak classifiers with strong discriminative power are then combined by the Multi-Class AdaBoost (AdaBoost.M2) algorithm. A dynamic programming algorithm is applied to segment and recognize actions simultaneously. Results of recognizing 22 actions on a large number of motion capture sequences as well as several annotated and automatically tracked sequences show the effectiveness of the proposed algorithms.},
	address = {Berlin, Heidelberg},
	author = {Lv, Fengjun and Nevatia, Ramakant},
	booktitle = {Computer Vision -- ECCV 2006},
	date-added = {2019-04-05 14:20:38 +0000},
	date-modified = {2019-04-05 14:20:55 +0000},
	editor = {Leonardis, Ale{\v{s}} and Bischof, Horst and Pinz, Axel},
	isbn = {978-3-540-33839-0},
	pages = {359--372},
	publisher = {Springer Berlin Heidelberg},
	title = {Recognition and Segmentation of 3-D Human Action Using HMM and Multi-class AdaBoost},
	year = {2006}}

@article{Santos_Khoshhal_Dias_2014,
	abstract = {This paper proposes a sliding window approach, whose length and time shift are dynamically adaptable in order to improve model confidence, speed and segmentation accuracy in human action sequences. Activity recognition is the process of inferring an action class from a set of observations acquired by sensors. We address the temporal segmentation problem of body part trajectories in Cartesian Space in which features are generated using Discrete Fast Fourier Transform (DFFT) and Power Spectrum (PS). We pose this as an entropy minimization problem. Using entropy from the classifier output as a feedback parameter, we continuously adjust the two key parameters in a sliding window approach, to maximize the model confidence at every step. The proposed classifier is a Dynamic Bayesian Network (DBN) model where classes are estimated using Bayesian inference. We compare our approach with our previously developed fixed window method. Experiments show that our method accurately recognizes and segments activities, with improved model confidence and faster convergence times, exhibiting anticipatory capabilities. Our work demonstrates that entropy feedback mitigates variability problems, and our method is applicable in research areas where action segmentation and classification is used. A working demo source code is provided online for academical dissemination purposes, by requesting the authors.},
	author = {Lu{\'\i}s Santos and Kamrad Khoshhal and Jorge Dias},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S003132031400329X},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.patcog.2014.08.015},
	date-added = {2019-04-05 14:18:42 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Motion segmentation, Classification framework, Signal processing, Motion variability, Adaptive sliding window},
	number = {2},
	pages = {568 - 579},
	title = {Trajectory-based human action segmentation},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S003132031400329X},
	volume = {48},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1016/j.patcog.2014.08.015}}

@inbook{Salamah_Zhang_Brunett_2015,
	abstract = {In this paper, we present a novel simple and efficient method for segmentation by classification of motion capture data automatically and with high accuracy. Classification of motion capture data demands dealing with high dimensional search space due to the high dimensionality of the motion capture data. The main contribution of this paper is a method for reducing this search space using the divide and conquer principle in a form of a taxonomy-tree which means a multi-level segmentation by classification algorithm, where the highest level classifies motion capture data into dynamic and static segments and the lowest level uses features of single body-parts to recognize wide range of human movements. The first implementation of this algorithm has given very promising results and proved that it is fast enough to be integrated in real-time systems such as robotics and surveillance systems.},
	address = {Cham},
	author = {Salamah, Samer and Zhang, Liang and Brunnett, Guido},
	bdsk-url_opt-1 = {https://doi.org/10.1007/978-3-319-17043-5_10},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/978-3-319-17043-5_10},
	booktitle = {Virtual Realities: International Dagstuhl Seminar, Dagstuhl Castle, Germany, June 9-14, 2013, Revised Selected Papers},
	date-added = {2019-04-05 14:16:20 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	editor = {Brunnett, Guido and Coquillart, Sabine and van Liere, Robert and Welch, Gregory and V{\'a}{\v{s}}a, Libor},
	isbn = {978-3-319-17043-5},
	pages = {169--186},
	publisher = {Springer International Publishing},
	title = {Hierarchical Method for Segmentation by Classification of Motion Capture Data},
	url_opt = {https://doi.org/10.1007/978-3-319-17043-5_10},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1007/978-3-319-17043-5_10}}

@article{Lan_Sun_2015,
	acmid = {2725258},
	address = {Secaucus, NJ, USA},
	author = {Lan, Rongyi and Sun, Huaijiang},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1007/s00371-013-0902-5},
	date-added = {2019-04-05 14:09:56 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0178-2789},
	issue_date = {January 2015},
	journal = {Vis. Comput.},
	keywords = {Hierarchical clustering, Latent Dirichlet allocation, Motion capture, Motion representation, Motion segmentation, Topic mining},
	month = jan,
	number = {1},
	numpages = {19},
	pages = {35--53},
	publisher = {Springer-Verlag New York, Inc.},
	title = {Automated Human Motion Segmentation via Motion Regularities},
	url_opt = {http://dx.doi.org/10.1007/s00371-013-0902-5},
	volume = {31},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s00371-013-0902-5}}

@article{Patrona_et_al_2017,
	abstract = {A novel framework, for real-time action detection, recognition and evaluation of motion capture data, is presented in this paper. Pose and kinematics information is used for data description. Automatic and dynamic weighting, altering joint data significance based on action involvement, and Kinetic energy-based descriptor sampling are employed for efficient action segmentation and labelling. The automatically segmented and recognized action instances are subsequently fed to the framework action evaluation component, which compares them with the corresponding reference ones, estimating their similarity. Exploiting fuzzy logic, the framework subsequently gives semantic feedback with instructions on performing the actions more accurately. Experimental results on MSR-Action3D and MSRC12 benchmarking datasets and a new, publicly available one, provide evidence that the proposed framework compares favourably to state-of-the-art methods by 0.5--6% in all three datasets, showing that the proposed method can be effectively used for unsupervised gesture/action training.},
	author = {Fotini Patrona and Anargyros Chatzitofis and Dimitrios Zarpalas and Petros Daras},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S0031320317304910},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.patcog.2017.12.007},
	date-added = {2019-04-05 14:05:42 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Online human action detection, Online human action recognition, Motion evaluation, Kinect, Skeleton data, Automatic joint/angle weighting, Kinetic energy},
	pages = {612 - 622},
	title = {Motion analysis: Action detection, recognition and evaluation based on motion capture data},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S0031320317304910},
	volume = {76},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/j.patcog.2017.12.007}}

@inproceedings{Meier_et_al_2011,
	abstract = {Segmenting complex movements into a sequence of primitives remains a difficult problem with many applications in the robotics and vision communities. In this work, we show how the movement segmentation problem can be reduced to a sequential movement recognition problem. To this end, we reformulate the original Dynamic Movement Primitive (DMP) formulation as a linear dynamical system with control inputs. Based on this new formulation, we develop an Expectation-Maximization algorithm to estimate the duration and goal position of a partially observed trajectory. With the help of this algorithm and the assumption that a library of movement primitives is present, we present a movement segmentation framework. We illustrate the usefulness of the new DMP formulation on the two applications of online movement recognition and movement segmentation.},
	author = {F. Meier and E. Theodorou and F. Stulp and S. Schaal},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/IROS.2011.6094676},
	booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	date-added = {2019-04-05 13:46:39 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {2153-0866},
	journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	journal1 = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	keywords = {Trajectory; Libraries; Motion segmentation; Covariance matrix; Noise; Equations},
	pages = {3407--3412},
	title = {Movement segmentation using a primitive library},
	ty = {CONF},
	year = {2011},
	year1 = {25-30 Sept. 2011},
	bdsk-url-1 = {http://dx.doi.org/10.1109/IROS.2011.6094676}}

@article{Krueger_et_al_2017,
	abstract = {We introduce a method for automated temporal segmentation of human motion data into distinct actions and compositing motion primitives based on self-similar structures in the motion sequence. We use neighborhood graphs for the partitioning and the similarity information in the graph is further exploited to cluster the motion primitives into larger entities of semantic significance. The method requires no assumptions about the motion sequences at hand and no user interaction is required for the segmentation or clustering. In addition, we introduce a feature bundling preprocessing technique to make the segmentation more robust to noise, as well as a notion of motion symmetry for more refined primitive detection. We test our method on several sensor modalities, including markered and markerless motion capture as well as on electromyograph and accelerometer recordings. The results highlight our system's capabilities for both segmentation and for analysis of the finer structures of motion data, all in a completely unsupervised manner.},
	author = {B. Kr{\"u}ger and A. V{\"o}gele and T. Willig and A. Yao and R. Klein and A. Weber},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/TMM.2016.2635030},
	booktitle = {IEEE Transactions on Multimedia},
	date-added = {2019-04-05 13:44:51 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {1520-9210},
	journal = {IEEE Transactions on Multimedia},
	journal1 = {IEEE Transactions on Multimedia},
	keywords = {graph theory; image motion analysis; image segmentation; image sequences; pattern clustering; unsupervised temporal segmentation; automated temporal segmentation; human motion data; motion primitives; motion sequence; neighborhood graphs; motion primitive clustering; semantic significance; feature bundling preprocessing technique; motion symmetry; primitive detection; sensor modalities; markered motion capture; electromyograph recordings; accelerometer recordings; Motion segmentation; Hidden Markov models; Data models; Computer vision; Principal component analysis; Robustness; Accelerometers; Temporal segmentation; time series clustering; human motion analysis},
	number = {4},
	pages = {797--812},
	title = {Efficient Unsupervised Temporal Segmentation of Motion Data},
	ty = {JOUR},
	vo = {19},
	volume = {19},
	year = {2017},
	year1 = {April 2017},
	bdsk-url-1 = {http://dx.doi.org/10.1109/TMM.2016.2635030}}

@inproceedings{Dreher_et_al_2017,
	abstract = {There have been many proposals for algorithms segmenting human whole-body motion in the literature. However, the wide range of use cases, datasets, and quality measures that were used for the evaluation render the comparison of algorithms challenging. In this paper, we introduce a framework that puts motion segmentation algorithms on a unified testing ground and provides a possibility to allow comparing them. The testing ground features both a set of quality measures known from the literature and a novel approach tailored to the evaluation of motion segmentation algorithms, termed Integrated Kernel approach. Datasets of motion recordings, provided with a ground truth, are included as well. They are labelled in a new way, which hierarchically organises the ground truth, to cover different use cases that segmentation algorithms can possess. The framework and datasets are publicly available and are intended to represent a service for the community regarding the comparison and evaluation of existing and new motion segmentation algorithms.},
	author = {C. R. G. Dreher and N. Kulp and C. Mandery and M. W{\"a}chter and T. Asfour},
	bdsk-url_opt-1 = {http://dx.doi.org/10.1109/HUMANOIDS.2017.8239541},
	booktitle = {2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)},
	date-added = {2019-04-05 13:40:17 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	isbn = {2164-0580},
	journal = {2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)},
	journal1 = {2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)},
	keywords = {image motion analysis; image segmentation; motion segmentation algorithms; quality measures; ground truth; human whole-body motion; unified testing ground; integrated kernel approach; motion recordings; use cases; Motion segmentation; Computer vision; Kernel; Measurement uncertainty; Principal component analysis; Hidden Markov models; Motion measurement},
	pages = {83--90},
	read = {0},
	title = {A framework for evaluating motion segmentation algorithms},
	ty = {CONF},
	year = {2017},
	year1 = {15-17 Nov. 2017},
	bdsk-url-1 = {http://dx.doi.org/10.1109/HUMANOIDS.2017.8239541}}

@inproceedings{Bernard_et_al_2017,
	abstract = {The characterization and abstraction of large multivariate time series data often poses challenges with respect to effectiveness or efficiency. Using the example of human motion capture data challenges exist in creating compact solutions that still reflect semantics and kinematics in a meaningful way. We present a visual-interactive approach for the semi-supervised labeling of human motion capture data. Users are enabled to assign labels to the data which can subsequently be used to represent the multivariate time series as sequences of motion classes. The approach combines multiple views supporting the user in the visual-interactive labeling process. Visual guidance concepts further ease the labeling process by propagating the results of supportive algorithmic models. The abstraction of motion capture data to sequences of event intervals allows overview and detail-on-demand visualizations even for large and heterogeneous data collections. The guided selection of candidate data for the extension and improvement of the labeling closes the feedback loop of the semi-supervised workflow. We demonstrate the effectiveness and the efficiency of the approach in two usage scenarios, taking visual-interactive learning and human motion synthesis as examples.},
	address = {USA / Vereinigte Staaten},
	author = {J{\"u}rgen Bernard and Eduard Dobermann and Anna V{\"o}gele and Bj{\"o}rn Kr{\"u}ger and J{\"o}rn Kohlhammer and Dieter Fellner},
	bdsk-url_opt-1 = {http://dx.doi.org/10.2352/ISSN.2470-1173.2017.1.VDA-387},
	booktitle = {Visualization and Data Analysis (VDA)},
	date-added = {2019-04-05 13:24:22 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	language = {undefiniert/unbekannt},
	pages = {34--45},
	publisher = {Society for Imaging Science and Technology},
	series = {Electronic Imaging},
	title = {Visual-Interactive Semi-Supervised Labeling of Human Motion Capture Data},
	year = {2017},
	bdsk-url-1 = {http://dx.doi.org/10.2352/ISSN.2470-1173.2017.1.VDA-387}}

@article{Krueger_et_al_2007,
	abstract = { In this paper, we analyze the different approaches taken to date within the computer vision, robotics and artificial intelligence communities for the representation, recognition, synthesis and understanding of action. We deal with action at different levels of complexity and provide the reader with the necessary related literature references. We put the literature references further into context and outline a possible interpretation of action by taking into account the different aspects of action recognition, action synthesis and task-level planning. },
	author = {Volker Kr{\"u}ger and Danica Kragic and Ale{\v s} Ude and Christopher Geib},
	bdsk-url_opt-1 = {https://www.tandfonline.com/doi/abs/10.1163/156855307782148578},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1163/156855307782148578},
	date-added = {2019-04-05 13:09:25 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	eprint_opt = {https://www.tandfonline.com/doi/pdf/10.1163/156855307782148578},
	journal = {Advanced Robotics},
	number = {13},
	pages = {1473-1501},
	publisher = {Taylor &amp; Francis},
	title = {The meaning of action: a review on action recognition and mapping},
	url_opt = {https://www.tandfonline.com/doi/abs/10.1163/156855307782148578},
	volume = {21},
	year = {2007},
	bdsk-url-1 = {http://dx.doi.org/10.1163/156855307782148578}}

@article{Lin_Karg_Kulic_2016,
	author = {Lin, Jonathan Feng-Shun and Karg, Michelle and Kuli{\'c}, Dana},
	date-added = {2019-04-05 12:51:42 +0000},
	date-modified = {2019-04-05 12:52:13 +0000},
	journal = {IEEE Transactions on Human-Machine Systems},
	number = {3},
	pages = {325--339},
	publisher = {IEEE},
	title = {Movement primitive segmentation for human motion modeling: A framework for analysis},
	volume = {46},
	year = {2016}}

@article{Lara_Labrador_2013,
	author = {Lara, Oscar D and Labrador, Miguel A},
	date-added = {2019-04-05 12:50:13 +0000},
	date-modified = {2019-04-05 12:50:29 +0000},
	journal = {IEEE Communications Surveys \&amp; Tutorials},
	number = {3},
	pages = {1192--1209},
	publisher = {IEEE},
	title = {A survey on human activity recognition using wearable sensors},
	volume = {15},
	year = {2013}}

@inproceedings{Ahad_et_al_2008,
	author = {Ahad, Md Atiqur Rahman and Tan, JK and Kim, HS and Ishikawa, S},
	booktitle = {2008 International Conference on Control, Automation and Systems},
	date-added = {2019-04-05 11:05:06 +0000},
	date-modified = {2019-04-05 11:05:27 +0000},
	organization = {IEEE},
	pages = {1896--1901},
	title = {Human activity recognition: Various paradigms},
	year = {2008}}

@article{Mitra_Acharya_2007,
	author = {Mitra, Sushmita and Acharya, Tinku},
	date-added = {2019-04-05 10:27:52 +0000},
	date-modified = {2019-04-05 10:28:25 +0000},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	number = {3},
	pages = {311--324},
	publisher = {IEEE},
	title = {Gesture recognition: A survey},
	volume = {37},
	year = {2007}}

@article{Zacks_et_al_2009,
	author = {Zacks, Jeffrey M and Kumar, Shawn and Abrams, Richard A and Mehta, Ritesh},
	date-added = {2019-04-04 14:31:07 +0000},
	date-modified = {2019-04-04 14:31:28 +0000},
	journal = {Cognition},
	number = {2},
	pages = {201--216},
	publisher = {Elsevier},
	title = {Using movement and intentions to understand human activity},
	volume = {112},
	year = {2009}}

@article{Blaesing_2015,
	author = {Bl{\"a}sing, Bettina E},
	date-added = {2019-04-04 14:28:49 +0000},
	date-modified = {2019-04-04 14:29:56 +0000},
	journal = {Frontiers in psychology},
	pages = {1500},
	publisher = {Frontiers},
	title = {Segmentation of dance movement: effects of expertise, visual familiarity, motor experience and music},
	volume = {5},
	year = {2015}}

@inproceedings{Kahol_Tripathi_Panchanathan_2004,
	author = {Kahol, Kanav and Tripathi, Priyamvada and Panchanathan, Sethuraman},
	booktitle = {Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.},
	date-added = {2019-04-04 14:26:34 +0000},
	date-modified = {2019-04-04 14:27:12 +0000},
	organization = {IEEE},
	pages = {883--888},
	title = {Automated gesture segmentation from dance sequences},
	year = {2004}}

@article{Hard_Tversky_Lang_2006,
	author = {Hard, Bridgette M and Tversky, Barbara and Lang, David S},
	date-added = {2019-04-04 14:24:52 +0000},
	date-modified = {2019-04-04 14:36:14 +0000},
	journal = {Memory \&amp; cognition},
	number = {6},
	pages = {1221--1235},
	publisher = {Springer},
	title = {Making sense of abstract events: Building event schemas},
	volume = {34},
	year = {2006}}

@article{Zacks_Tversky_Iyer_2001,
	author = {Zacks, Jeffrey M and Tversky, Barbara and Iyer, Gowri},
	date-added = {2019-04-04 14:23:26 +0000},
	date-modified = {2019-04-04 14:23:51 +0000},
	journal = {Journal of Experimental Psychology: General},
	number = {1},
	pages = {29},
	publisher = {American Psychological Association},
	title = {Perceiving, remembering, and communicating structure in events.},
	volume = {130},
	year = {2001}}

@article{Newtson_1973,
	author = {Newtson, Darren},
	date-added = {2019-04-04 14:21:16 +0000},
	date-modified = {2019-04-04 14:21:53 +0000},
	journal = {Journal of Personality and Social Psychology},
	number = {1},
	pages = {28},
	publisher = {American Psychological Association},
	title = {Attribution and the unit of perception of ongoing behavior.},
	volume = {28},
	year = {1973}}

@inproceedings{Rodrigues_Probst_Gamboa_2021,
	abstract = {An interactive loop between motion recognition and motion generation is a fundamental mechanism for humans and humanoid robots. We have been developing an intelligent framework for motion recognition and generation based on symbolizing motion primitives. The motion primitives are encoded into Hidden Markov Models (HMMs), which we call ``motion symbols''. However, to determine the motion primitives to use as training data for the HMMs, this framework requires a manual segmentation of human motions. Essentially, a humanoid robot is expected to participate in daily life and must learn many motion symbols to adapt to various situations. For this use, manual segmentation is cumbersome and impractical for humanoid robots. In this study, we propose a novel approach to segmentation, the Real-time Unsupervised Segmentation (RUS) method, which comprises three phases. In the first phase, short human movements are encoded into feature HMMs. Seamless human motion can be converted to a sequence of these feature HMMs. In the second phase, the causality between the feature HMMs is extracted. The causality data make it possible to predict movement from observation. In the third phase, movements having a large prediction uncertainty are designated as the boundaries of motion primitives. In this way, human whole-body motion can be segmented into a sequence of motion primitives. This paper also describes an application of RUS to AUtonomous Symbolization of motion primitives (AUS). Each derived motion primitive is classified into an HMM for a motion symbol, and parameters of the HMMs are optimized by using the motion primitives as training data in competitive learning. The HMMs are gradually optimized in such a way that the HMMs can abstract similar motion primitives. We tested the RUS and AUS frameworks on captured human whole-body motions and demonstrated the validity of the proposed framework.},
	author = {Jo{\~a}o Rodrigues and Phillip Probst and Hugo Gamboa},
	booktitle = {2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)},
	date-added = {2019-04-04 14:00:27 +0000},
	date-modified = {2021-11-01 19:49:26 +0200},
	pages = {1-6},
	title = {TSSummarize: A Visual Strategy to Summarize Biosignals},
	url = {https://ieeexplore.ieee.org/document/9445154},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1016/j.robot.2015.09.021}}

@article{Paliyawan_Choensawat_Thawonmas_2018,
	abstract = {This paper presents a novel approach for motion segmentation by using strategies of splitting and remerging. The presented approach, Mossar, hybridizes two existing ones to obtain their potential advantages while covering weaknesses: (1) velocity-based, one of the most widely used approaches that has fairly low accuracy but provides computational simplicity and (2) graph-based, a state-of-the-art approach that provides outstanding accuracy, yet bears high computational complexity and a burden in setting of thresholds. An initial set of key frames is generated by a velocity-based splitting process and then fed into a graph-based remerging process for refinement. We present mechanisms that improve key-frames capturing in the velocity-based approach as well as details on how the graph-based approach is modified and later applied to remerging. The proposed approach also allows users to interactively add or reduce the number of key frames to control segmentation hierarchy without the need to change threshold values and re-run segmentation, as usually done in existing approaches. Our experimental results show that the presented hybrid approach, compared to both velocity-based and graph-based, demonstrates superior performance in terms of accuracy and in comparison to graph-based, our approach has not only less complexity but also a lesser number of thresholds, the values of which can be much more simply determined.},
	author = {Paliyawan, Pujana and Choensawat, Worawat and Thawonmas, Ruck},
	bdsk-url_opt-1 = {https://doi.org/10.1007/s11042-018-5965-x},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/s11042-018-5965-x},
	date-added = {2019-04-04 13:54:52 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	day = {01},
	issn = {1573-7721},
	journal = {Multimedia Tools and Applications},
	month = {Nov},
	number = {21},
	pages = {27761--27788},
	title = {Mossar: motion segmentation by using splitting and remerging strategies},
	url_opt = {https://doi.org/10.1007/s11042-018-5965-x},
	volume = {77},
	year = {2018},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s11042-018-5965-x}}

@article{Regazzoni_de_Vecchi_Rizzi_2014,
	abstract = {Motion capture of the human body has being performed for decades with a growing number of technologies, aims and application fields; but only recent optical markerless technologies based on silhouette recognition and depth sensors which have been developed for videogames control interface have brought motion capture to a broad diffusion. Actually, nowadays there are low cost hardware and software suitable for a wide range of applications that may vary from entertainment domain (e.g., videogames, virtual characters in movies) to the biomechanical and biomedical domain (e.g., gait analysis or orthopedic rehabilitation) and to a huge number of industrial sectors. In this quick evolving scenario it is hard to tell which technology is the most suitable for any desired goal. The aim of the paper is to answer to this issue by presenting a benchmark analysis that compares RGB and RGB-D technologies used to track performing people in a variety of conditions. In order to contrast the solutions, several different tasks have been selected, simultaneously captured and post-processed exactly in the same way. The test campaign has been designed to evaluate pros and cons according to the most important feature of a motion capture technology, such as volume of acquisition, accuracy of joint position and tracking of fast movements. Actors were asked to perform a number of tasks, among which free movements of arms, legs and full body, gait, and tasks performed interacting with a machine. The number of sensors around the scene and their disposition have been considered as well. We used Sony PS Eye cameras and Microsoft Kinect sensors as hardware solutions and iPisoft for data elaboration. The gathered results are organized, compared and discussed stressing performances and limitations of any combination and, at last, we proposed the best candidate technology for some key applications.},
	author = {Daniele Regazzoni and Giordano de Vecchi and Caterina Rizzi},
	bdsk-url_opt-1 = {http://www.sciencedirect.com/science/article/pii/S0278612514000910},
	bdsk-url_opt-2 = {https://doi.org/10.1016/j.jmsy.2014.07.011},
	date-added = {2019-04-04 11:33:47 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	issn = {0278-6125},
	journal = {Journal of Manufacturing Systems},
	keywords = {RGB, RGB-D, Depth cameras, Motion capture, Digital Human Models, Benchmark},
	number = {4},
	pages = {719 - 728},
	title = {RGB cams vs RGB-D sensors: Low cost motion capture technologies performances and limitations},
	url_opt = {http://www.sciencedirect.com/science/article/pii/S0278612514000910},
	volume = {33},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1016/j.jmsy.2014.07.011}}

@article{Van_der_Kruk_Reijne_2018,
	abstract = { AbstractObjective: Sport research often requires human motion capture of an athlete. It can, however, be labour-intensive and difficult to select the right system, while manufacturers report on specifications which are determined in set-ups that largely differ from sport research in terms of volume, environment and motion. The aim of this review is to assist researchers in the selection of a suitable motion capture system for their experimental set-up for sport applications. An open online platform is initiated, to support (sport)researchers in the selection of a system and to enable them to contribute and update the overview. Design: systematic review; Method: Electronic searches in Scopus, Web of Science and Google Scholar were performed, and the reference lists of the screened articles were scrutinised to determine human motion capture systems used in academically published studies on sport analysis. Results: An overview of 17 human motion capture systems is provided, reporting the general specifications given by the manufacturer (weight and size of the sensors, maximum capture volume, environmental feasibilities), and calibration specifications as determined in peer-reviewed studies. The accuracy of each system is plotted against the measurement range. Conclusion: The overview and chart can assist researchers in the selection of a suitable measurement system. To increase the robustness of the database and to keep up with technological developments, we encourage researchers to perform an accuracy test prior to their experiment and to add to the chart and the system overview (online, open access). },
	author = {Eline van der Kruk and Marco M. Reijne},
	bdsk-url_opt-1 = {https://doi.org/10.1080/17461391.2018.1463397},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1080/17461391.2018.1463397},
	date-added = {2019-04-04 11:30:57 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	eprint_opt = {https://doi.org/10.1080/17461391.2018.1463397},
	journal = {European Journal of Sport Science},
	note = {PMID: 29741985},
	number = {6},
	pages = {806-819},
	publisher = {Routledge},
	title = {Accuracy of human motion capture systems for sport applications; state-of-the-art review},
	url_opt = {https://doi.org/10.1080/17461391.2018.1463397},
	volume = {18},
	year = {2018},
	bdsk-url-1 = {http://dx.doi.org/10.1080/17461391.2018.1463397}}

@article{Xia_et_al_2017,
	abstract = {With the rapid development of computing technology, three-dimensional (3D) human body models and their dynamic motions are widely used in the digital entertainment industry. Human performance mainly involves human body shapes and motions. Key research problems in human performance animation include how to capture and analyze static geometric appearance and dynamic movement of human bodies, and how to simulate human body motions with physical effects. In this survey, according to the main research directions of human body performance capture and animation, we summarize recent advances in key research topics, namely human body surface reconstruction, motion capture and synthesis, as well as physics-based motion simulation, and further discuss future research problems and directions. We hope this will be helpful for readers to have a comprehensive understanding of human performance capture and animation.},
	author = {Xia, Shihong and Gao, Lin and Lai, Yu-Kun and Yuan, Ming-Ze and Chai, Jinxiang},
	bdsk-url_opt-1 = {https://doi.org/10.1007/s11390-017-1742-y},
	bdsk-url_opt-2 = {http://dx.doi.org/10.1007/s11390-017-1742-y},
	date-added = {2019-04-04 11:25:32 +0000},
	date-modified = {2020-05-17 02:14:14 +0000},
	day = {01},
	issn = {1860-4749},
	journal = {Journal of Computer Science and Technology},
	month = {May},
	number = {3},
	pages = {536--554},
	title = {A Survey on Human Performance Capture and Animation},
	url_opt = {https://doi.org/10.1007/s11390-017-1742-y},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {http://dx.doi.org/10.1007/s11390-017-1742-y}}
